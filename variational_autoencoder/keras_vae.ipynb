{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow v2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Reshape, Dense, Input, Flatten, Lambda\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import Model\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "print(f\"Using Tensorflow v{tf.__version__}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Set up the encoder input\n",
    "encoder_input_shape = (1,256,1)\n",
    "encoder_input = Input(shape=encoder_input_shape, name=\"encoder_input\")\n",
    "# First convolution layers\n",
    "encoder_conv2D_1 = Conv2D(\n",
    "    filters=16,\n",
    "    kernel_size=(4,1),\n",
    "    strides=(1,2),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    name=\"encoder_conv2d_1\"\n",
    ")(encoder_input)\n",
    "\n",
    "# Second convolution layer\n",
    "encoder_conv2D_2 = Conv2D(\n",
    "    filters=16,\n",
    "    kernel_size=(4,1),\n",
    "    strides=(1,2),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    name=\"encoder_conv2d_2\"\n",
    ")(encoder_conv2D_1)\n",
    "\n",
    "# Reshape \n",
    "encoder_output = Reshape((1,-1), name=\"encoder_reshape\")(encoder_conv2D_2)\n",
    "\n",
    "# Dense\n",
    "# encoder_output = Dense(16, name=\"mu\")(encoder_output)\n",
    "\n",
    "encoder = Model(encoder_input, encoder_output, name=\"encoder\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_shape = (1,16,1)\n",
    "decoder_input = Input(shape=decoder_input_shape, name=\"decoder_input\")\n",
    "decoder_dense = Dense(128, name='decoder_dense_1')(decoder_input)\n",
    "decoder_reshape = Reshape((1,64, -1), name=\"decoder_reshape\")(decoder_dense)\n",
    "decoder_conv2dT_1 = Conv2DTranspose(\n",
    "    filters=16,\n",
    "    kernel_size=(4,1),\n",
    "    strides=(1,2),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    name=\"decoder_conv2dT_1\"\n",
    ")(decoder_reshape)\n",
    "decoder_output = Conv2DTranspose(\n",
    "    filters=1,\n",
    "    kernel_size=(4,1),\n",
    "    strides=(1,2),\n",
    "    padding=\"same\",\n",
    "    activation=\"sigmoid\",\n",
    "    name=\"decoder_output\"\n",
    ")(decoder_conv2dT_1)\n",
    "decoder = Model(decoder_input, decoder_output, name=\"decoder\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model._should_eval of <keras.engine.functional.Functional object at 0x28de36560>>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.ev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    wave_size = 256\n",
    "    variational_beta = 0.9\n",
    "    def __init__(self, encoder, decoder, encoder_input, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.training = True\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.recon_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "    \n",
    "    def summary(self):\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.recon_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    def sample_normal_point(self, mu, log_var):\n",
    "        if self.training:\n",
    "            epsilon = K.random_normal(shape=K.shape(mu), mean=0., stddev=1.)\n",
    "            sampled_point = mu + K.exp(log_var/2) * epsilon\n",
    "            return sampled_point\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate the reconstructed data\n",
    "            self.mu =  Dense(16, name=\"mu\")(self.encoder(data))\n",
    "            self.log_var = Dense(16, name=\"log_var\")(self.encoder(data))\n",
    "            sampled_point = self.sample_normal_point(self.mu, self.log_var)\n",
    "            recon_x = self.decoder(sampled_point)\n",
    "            # Generate the reconstruction loss\n",
    "            recon_loss = binary_crossentropy(tf.reshape(data, (-1,self.wave_size)), \n",
    "                                                        tf.reshape(recon_x, (-1,self.wave_size)))\n",
    "            recon_loss = tf.reduce_mean(tf.reduce_sum(recon_loss))\n",
    "            \n",
    "            # Generate the kl divergence\n",
    "            kl_loss = -0.5 * (1 + self.log_var - tf.square(self.mu) - tf.exp(self.log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss))\n",
    "\n",
    "            # Total variation loss\n",
    "            tv_loss =  tf.reduce_sum(tf.square(recon_x[:,:,:-1] - recon_x[:,:,1:]))\n",
    "\n",
    "            # Calculate the total loss\n",
    "            total_loss = recon_loss + self.variational_beta * kl_loss + 100.01 * tv_loss\n",
    "\n",
    "        # Generate the gradients\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.recon_loss_tracker.update_state(recon_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"total_loss\": self.total_loss_tracker.result(),\n",
    "            \"recon_loss\": self.recon_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result()\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tensor):\n",
    "    tensor_minusmin = tensor - tensor.min()\n",
    "    tensor_maxminusmin = tensor.max() - tensor.min()\n",
    "    return tensor_minusmin / tensor_maxminusmin\n",
    "\n",
    "def get_waves(file_name):\n",
    "    # Read the raw audio from the .WAV file\n",
    "    raw_audio = tf.io.read_file(filename=file_name)\n",
    "    # Convert the raw audio to a waveform\n",
    "    wave_bank, sample_rate = tf.audio.decode_wav(raw_audio)\n",
    "    # Display the wavebank and sample_rate\n",
    "    # display(wave_bank, sample_rate)\n",
    "    wave_size = 256\n",
    "    num_waves = 0\n",
    "    waves = []\n",
    "    wave_forms = np.transpose(wave_bank.numpy())\n",
    "    for i in range(64):\n",
    "        wave_form = wave_forms[0, i*wave_size:i*wave_size+wave_size]\n",
    "        if wave_form.max() != wave_form.min():\n",
    "            wave_form = normalize(wave_form)\n",
    "            waves.append(wave_form)\n",
    "            num_waves += 1    \n",
    "    waves = tf.stack(waves)\n",
    "    return waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 256])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 1, 1, 256, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "waves = get_waves(\"./audio_data/ENVELO01.WAV\")\n",
    "display(waves.shape)\n",
    "waves = tf.expand_dims(waves,1)\n",
    "waves = tf.expand_dims(waves,1)\n",
    "waves = tf.expand_dims(waves,-1)\n",
    "display(waves.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 1, 256, 1)]       0         \n",
      "                                                                 \n",
      " encoder_conv2d_1 (Conv2D)   (None, 1, 128, 16)        80        \n",
      "                                                                 \n",
      " encoder_conv2d_2 (Conv2D)   (None, 1, 64, 16)         1040      \n",
      "                                                                 \n",
      " encoder_reshape (Reshape)   (None, 1, 1024)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,120\n",
      "Trainable params: 1,120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 1, 16, 1)]        0         \n",
      "                                                                 \n",
      " decoder_dense_1 (Dense)     (None, 1, 16, 128)        256       \n",
      "                                                                 \n",
      " decoder_reshape (Reshape)   (None, 1, 64, 32)         0         \n",
      "                                                                 \n",
      " decoder_conv2dT_1 (Conv2DTr  (None, 1, 128, 16)       2064      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " decoder_output (Conv2DTrans  (None, 1, 256, 1)        65        \n",
      " pose)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,385\n",
      "Trainable params: 2,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(encoder=encoder, decoder=decoder, encoder_input=encoder_input)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:33<00:00,  3.39s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for j in tqdm(range(num_epochs)):\n",
    "    for i in waves:\n",
    "        loss = vae.train_step(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "vae.decoder.save(\"./keras_vae.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff1ec9c71d6c6569ddb6931262fd540c04a3bd4bd27a165637ca0df5de8b220f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
