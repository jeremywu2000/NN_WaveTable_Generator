{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set up imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "print(tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set up loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_recon_loss(y_target, y_pred):\n",
    "#     error = y_target - y_pred\n",
    "#     recon_loss = np.mean(np.sum(error), axis=[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dummy Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "Model: \"Encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 1, 128, 16)        80        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 64, 32)         2080      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 2048)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1, 16)             32784     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,944\n",
      "Trainable params: 34,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 15:16:45.349139: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-02-20 15:16:45.349349: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "encoder = tf.keras.models.Sequential(name=\"Encoder\")\n",
    "encoder.add(tf.keras.layers.Conv2D(16, kernel_size=(4,1), strides=(1,2), padding='same', activation='relu', input_shape=(1,256,1)))\n",
    "encoder.add(tf.keras.layers.Conv2D(32, kernel_size=(4,1), strides=(1,2), padding='same', activation='relu'))\n",
    "encoder.add(tf.keras.layers.Reshape((1,-1)))\n",
    "encoder.add(tf.keras.layers.Dense(16))\n",
    "\n",
    "encoder.build()\n",
    "encoder.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Dummy Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 1, 16, 128)        256       \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 1, 64, 32)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 1, 128, 16)       2064      \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 1, 256, 1)        65        \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,385\n",
      "Trainable params: 2,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder = tf.keras.models.Sequential(name=\"Decoder\")\n",
    "decoder.add(tf.keras.layers.Dense(128, input_shape=(1,16,1)))\n",
    "decoder.add(tf.keras.layers.Reshape((1,64,-1)))\n",
    "decoder.add(tf.keras.layers.Conv2DTranspose(16, kernel_size=(4,1),strides=(1,2), padding='same', activation='relu'))\n",
    "decoder.add(tf.keras.layers.Conv2DTranspose(1, kernel_size=(4,1),strides=(1,2), padding='same', activation='sigmoid'))\n",
    "# decoder.add(tf.keras.layers.Dense(16*128, input_shape=(1,16,1)))\n",
    "\n",
    "\n",
    "decoder.build()\n",
    "decoder.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    def __init__(self):\n",
    "        # Initialize the input shape\n",
    "        self.input_shape = (1,256,1)\n",
    "\n",
    "        # Initialize the encoder\n",
    "        self.encoder = tf.keras.models.Sequential(name=\"Encoder\")\n",
    "        self.encoder.add(tf.keras.layers.Conv2D(16, kernel_size=(4,1), strides=(1,2), padding='same', activation='relu', input_shape=self.input_shape))\n",
    "        self.encoder.add(tf.keras.layers.Conv2D(32, kernel_size=(4,1), strides=(1,2), padding='same', activation='relu'))\n",
    "        self.encoder.add(tf.keras.layers.Reshape((1,-1)))\n",
    "        self.encoder.add(tf.keras.layers.Dense(16))\n",
    "\n",
    "        # Initialize the decoder\n",
    "        self.decoder = tf.keras.models.Sequential(name=\"Decoder\")\n",
    "        self.decoder.add(tf.keras.layers.Dense(128, input_shape=(1,16,1)))\n",
    "        self.decoder.add(tf.keras.layers.Reshape((1,64,-1)))\n",
    "        self.decoder.add(tf.keras.layers.Conv2DTranspose(16, kernel_size=(4,1),strides=(1,2), padding='same', activation='relu'))\n",
    "        self.decoder.add(tf.keras.layers.Conv2DTranspose(1, kernel_size=(4,1),strides=(1,2), padding='same', activation='sigmoid'))\n",
    "\n",
    "        # Intialize VAE model\n",
    "        model_input = tf.keras.layers.Input(shape=self.input_shape, name=\"encoder_input\")\n",
    "        model_output = self.decoder(self.encoder(model_input))\n",
    "        self.model = tf.keras.models.Model(model_input, model_output, name=\"Autoencoder\")\n",
    "\n",
    "    def summary(self):\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "        self.model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 1, 128, 16)        80        \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 1, 64, 32)         2080      \n",
      "                                                                 \n",
      " reshape_6 (Reshape)         (None, 1, 2048)           0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1, 16)             32784     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,944\n",
      "Trainable params: 34,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, 1, 16, 128)        256       \n",
      "                                                                 \n",
      " reshape_7 (Reshape)         (None, 1, 64, 32)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_6 (Conv2DT  (None, 1, 128, 16)       2064      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_7 (Conv2DT  (None, 1, 256, 1)        65        \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,385\n",
      "Trainable params: 2,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 1, 256, 1)]       0         \n",
      "                                                                 \n",
      " Encoder (Sequential)        (None, 1, 16)             34944     \n",
      "                                                                 \n",
      " Decoder (Sequential)        (None, 1, 256, 1)         2385      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,329\n",
      "Trainable params: 37,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae = VAE()\n",
    "vae.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train the Variational Autoencoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Export the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "decoder.save('./keras_decoder_untrained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def __init__(self):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         c = capacity\n",
    "#         self.fc = nn.Linear(in_features=latent_dims, out_features=c*wave_sized2)\n",
    "#         self.conv2 = nn.ConvTranspose1d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
    "#         self.conv1 = nn.ConvTranspose1d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
    "            \n",
    "#     def forward(self, x):\n",
    "#         x = self.fc(x)\n",
    "#         x = x.view(x.size(0), capacity*2, wave_sized4) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
    "# #        x = x.view(1, 32, 64) #### need to hardcode the shape in view to satisfy loading in onnx.js !!! ####\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = torch.sigmoid(self.conv1(x)) # last layer before output is sigmoid, since we are using BCE as reconstruction loss\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 16:02:47.461506: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 389ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 16:02:47.888015: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 801ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.04927252,  0.00486319,  0.01706034, -0.09054629,\n",
       "         -0.08896114, -0.00797448,  0.01536477, -0.04613001,\n",
       "          0.06059843, -0.00744831, -0.06729551,  0.10568736,\n",
       "         -0.05514535, -0.02591524, -0.05836235,  0.02622267]]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1, 1, 16)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_input = np.random.randn(1,1,256)\n",
    "example_input.shape\n",
    "en_pred = encoder.predict(example_input)\n",
    "new_pred = decoder.predict(en_pred)\n",
    "display(en_pred, en_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff1ec9c71d6c6569ddb6931262fd540c04a3bd4bd27a165637ca0df5de8b220f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
