{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.optim import Adam\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_size = 256\n",
    "latent_dims = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the wav paths\n",
    "wav_dir = \"./WAV/Various-ups-downs/\"\n",
    "wav_file_name = \"VARIOUS_.WAV\"\n",
    "wav_file = os.path.join(wav_dir, wav_file_name)\n",
    "\n",
    "# Setup tensor for waves\n",
    "waves = torch.empty(size=(wave_size,64))\n",
    "\n",
    "# Load the wav file\n",
    "wavebank, sample_rate = torchaudio.load(wav_file)\n",
    "\n",
    "# Set the waves' tensor indexer\n",
    "num_waves = 0\n",
    "\n",
    "# Normalizes data per for each wave\n",
    "def normalize(wave):\n",
    "    return (wave - wave.min()) / (wave.max() - wave.min())\n",
    "\n",
    "# Load the wav dataset into waves tensor\n",
    "for i in range(64):\n",
    "    waveform = wavebank[0, i*wave_size: i*wave_size+wave_size]\n",
    "    if waveform.max() != waveform.min():\n",
    "        waveform = normalize(waveform)\n",
    "        waves[:,num_waves] = waveform\n",
    "        num_waves += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Variational Autoencoder Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the convolutional layer parameters\n",
    "capacity = 16\n",
    "kernel = 4\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "# Number of output channels at each convolutional layer\n",
    "conv1_out_channels = capacity\n",
    "conv2_out_channels = 2 * capacity\n",
    "\n",
    "# Finds the width of the convolution output\n",
    "def conv1DOutWidth(in_w, kernel_w=kernel, stride_w=stride, padding_w=padding):\n",
    "    new_w = (in_w + 2*padding_w - kernel_w)/(stride_w) + 1\n",
    "    return int(new_w)\n",
    "\n",
    "# Convolution output widths\n",
    "conv1_out_w = conv1DOutWidth(in_w=wave_size)\n",
    "conv2_out_w = conv1DOutWidth(in_w=conv1_out_w)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        c = capacity\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc_mu = nn.Linear(in_features=c*2*conv2_out_w, out_features=latent_dims)\n",
    "        self.fc_logvar = nn.Linear(in_features=c*2*conv2_out_w, out_features=latent_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_mu = self.fc_mu(x)\n",
    "        x_logvar = self.fc_logvar(x)\n",
    "        return x_mu, x_logvar\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        c = capacity\n",
    "        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*conv2_out_w)\n",
    "        self.conv2 = nn.ConvTranspose1d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv1 = nn.ConvTranspose1d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), conv2_out_channels, conv2_out_w)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.sigmoid(self.conv1(x)) # last layer before output is sigmoid, since we are using BCE as reconstruction loss\n",
    "        return x\n",
    "\n",
    "class LitVariationalAutoencoder(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.loss = []\n",
    "        self.recon_loss = []\n",
    "        self.learning_rate = 1e-3\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent_mu, latent_logvar = self.encoder(x)\n",
    "        latent = self.latent_sample(latent_mu, latent_logvar)\n",
    "        x_recon = self.decoder(latent)\n",
    "        return x_recon, latent_mu, latent_logvar\n",
    "    \n",
    "    def latent_sample(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # the reparameterization trick\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = torch.empty_like(std).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def training_step(self, wave_batch, batch_idx):\n",
    "        wave_batch = wave_batch.unsqueeze(1)\n",
    "        wave_batch_recon, latent_mu, latent_logvar = self(wave_batch)\n",
    "        loss = vae_loss(wave_batch_recon, wave_batch, latent_mu, latent_logvar)\n",
    "        self.log('training loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.loss.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=self.learning_rate, weight_decay=1e-5)        \n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    variational_beta = 0.1\n",
    "    recon_loss = F.binary_cross_entropy(recon_x.view(-1, wave_size), x.view(-1, wave_size), reduction='mean')\n",
    "    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    # Total variation loss.\n",
    "    tv_loss = torch.sum(torch.pow(recon_x[:,:,:-1] - recon_x[:,:,1:], 2))\n",
    "    return recon_loss + variational_beta * kldivergence + 100.01 * tv_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Set up dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledTensorDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping unlabeled data tensors (autoencoders do not need labels).\n",
    "\n",
    "    Each sample will be retrieved by indexing tensors along the second\n",
    "    dimension.\n",
    "\n",
    "    Arguments:\n",
    "        data_tensor (Tensor): contains sample data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_tensor, num_waves):\n",
    "        self.data_tensor = data_tensor\n",
    "        self.samples = list(range(0, num_waves))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_tensor[:,index]\n",
    "\n",
    "train_dataset = UnlabeledTensorDataset(waves[:,0:num_waves], num_waves)\n",
    "\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 67.7 K\n",
      "1 | decoder | Decoder | 36.9 K\n",
      "------------------------------------\n",
      "104 K     Trainable params\n",
      "0         Non-trainable params\n",
      "104 K     Total params\n",
      "0.419     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 64/64 [00:00<00:00, 84.42it/s, loss=1.41, v_num=13]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 64/64 [00:00<00:00, 83.18it/s, loss=1.41, v_num=13]\n"
     ]
    }
   ],
   "source": [
    "vae = LitVariationalAutoencoder()\n",
    "trainer = Trainer(accelerator='gpu', devices=1, max_epochs=50)\n",
    "trainer.fit(vae, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Export as ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%modelInput : Float(1, 16, strides=[16, 1], requires_grad=0, device=cpu),\n",
      "      %fc.weight : Float(2048, 16, strides=[16, 1], requires_grad=1, device=cpu),\n",
      "      %fc.bias : Float(2048, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2.weight : Float(32, 16, 4, strides=[64, 4, 1], requires_grad=1, device=cpu),\n",
      "      %conv2.bias : Float(16, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv1.weight : Float(16, 1, 4, strides=[4, 4, 1], requires_grad=1, device=cpu),\n",
      "      %conv1.bias : Float(1, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %/fc/Gemm_output_0 : Float(1, 2048, strides=[2048, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc/Gemm\"](%modelInput, %fc.weight, %fc.bias), scope: __main__.Decoder::/torch.nn.modules.linear.Linear::fc # /home/jeremy/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/Constant_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=  1  32  64 [ CPULongType{3} ], onnx_name=\"/Constant\"](), scope: __main__.Decoder:: # /tmp/ipykernel_3992/1867482986.py:47:0\n",
      "  %/Reshape_output_0 : Float(1, 32, 64, strides=[2048, 64, 1], requires_grad=1, device=cpu) = onnx::Reshape[onnx_name=\"/Reshape\"](%/fc/Gemm_output_0, %/Constant_output_0), scope: __main__.Decoder:: # /tmp/ipykernel_3992/1867482986.py:47:0\n",
      "  %/conv2/ConvTranspose_output_0 : Float(1, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = onnx::ConvTranspose[dilations=[1], group=1, kernel_shape=[4], pads=[1, 1], strides=[2], onnx_name=\"/conv2/ConvTranspose\"](%/Reshape_output_0, %conv2.weight, %conv2.bias), scope: __main__.Decoder::/torch.nn.modules.conv.ConvTranspose1d::conv2 # /home/jeremy/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:801:0\n",
      "  %/Relu_output_0 : Float(1, 16, 128, strides=[2048, 128, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/Relu\"](%/conv2/ConvTranspose_output_0), scope: __main__.Decoder:: # /home/jeremy/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n",
      "  %/conv1/ConvTranspose_output_0 : Float(1, 1, 256, strides=[256, 256, 1], requires_grad=0, device=cpu) = onnx::ConvTranspose[dilations=[1], group=1, kernel_shape=[4], pads=[1, 1], strides=[2], onnx_name=\"/conv1/ConvTranspose\"](%/Relu_output_0, %conv1.weight, %conv1.bias), scope: __main__.Decoder::/torch.nn.modules.conv.ConvTranspose1d::conv1 # /home/jeremy/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:801:0\n",
      "  %modelOutput : Float(1, 1, 256, strides=[256, 256, 1], requires_grad=1, device=cpu) = onnx::Sigmoid[onnx_name=\"/Sigmoid\"](%/conv1/ConvTranspose_output_0), scope: __main__.Decoder:: # /tmp/ipykernel_3992/1867482986.py:49:0\n",
      "  return (%modelOutput)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "\n",
    "dummy_input = torch.randn(1,latent_dims)\n",
    "vae.decoder.eval()\n",
    "torch.onnx.export(\n",
    "    vae.decoder,\n",
    "    dummy_input,\n",
    "    \"./test_model.onnx\",\n",
    "    verbose=True,\n",
    "    input_names=['modelInput'],\n",
    "    output_names=['modelOutput'],\n",
    "    opset_version=11,\n",
    "    export_params=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9593775b4d513f3b840c8c3b9515d475a61fb72e7d93f2015a3e30ae9402c003"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
